import os
import re
import time
import requests
import numpy as np
from datetime import datetime, timedelta, timezone
from collections import Counter
from dotenv import load_dotenv
from supabase import create_client

# ===============================
# â‘  .envã®èª­ã¿è¾¼ã¿ã¨è¨­å®š
# ===============================
load_dotenv()

EBAY_ACCESS_TOKEN = os.getenv("EBAY_ACCESS_TOKEN")
MARKETPLACE_ID = os.getenv("EBAY_MARKETPLACE_ID", "EBAY_US")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
supabase = None
if SUPABASE_URL and SUPABASE_KEY:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# ===============================
# â‘¡ æ¤œç´¢æœŸé–“è¨­å®šï¼ˆéå»90æ—¥ï¼‰
# ===============================
end_date = datetime.now(timezone.utc)
start_date = end_date - timedelta(days=90)
start_str = start_date.strftime("%Y-%m-%dT%H:%M:%SZ")
end_str = end_date.strftime("%Y-%m-%dT%H:%M:%SZ")

# ===============================
# â‘¢ eBay APIã®å…±é€šè¨­å®š
# ===============================
BASE_URL = "https://api.ebay.com/buy/browse/v1/item_summary/search"
COMMON_FILTER = f"itemLocationCountry:JP,soldDate:[{start_str}..{end_str}],price:[1..20000],buyingOptions:FIXED_PRICE"
HEADERS = {
    "Authorization": f"Bearer {EBAY_ACCESS_TOKEN}",
    "Content-Type": "application/json",
    "X-EBAY-C-MARKETPLACE-ID": MARKETPLACE_ID
}

# ===============================
# â‘£ ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã§è²©å£²ãƒ‡ãƒ¼ã‚¿å–å¾—
# ===============================
def fetch_all_items(category_id="183454", limit=100, max_pages=10):
    all_items = []
    offset = 0

    for page in range(max_pages):
        params = {
            "category_ids": category_id,
            "filter": COMMON_FILTER,
            "limit": str(limit),
            "offset": str(offset)
        }

        print(f"ğŸ“¦ ãƒšãƒ¼ã‚¸ {page + 1} ã‚’å–å¾—ä¸­... (offset={offset})")
        res = requests.get(BASE_URL, headers=HEADERS, params=params)

        if res.status_code != 200:
            print("âš ï¸ APIã‚¨ãƒ©ãƒ¼:", res.text)
            break

        data = res.json()
        items = data.get("itemSummaries", [])
        if not items:
            print("ğŸ”š ãƒ‡ãƒ¼ã‚¿å–å¾—çµ‚äº†ã€‚")
            break

        all_items.extend(items)
        offset += limit
        time.sleep(1)

        if len(items) < limit:
            break

    print(f"âœ… ç·å–å¾—ä»¶æ•°: {len(all_items)} ä»¶")
    return all_items

# ===============================
# â‘¤ Supabaseã«ä¿å­˜
# ===============================
def save_sales_data(category, total, avg, median, min_price, top_keywords, top_characters):
    """Supabaseã«åˆ†æçµæœã‚’ä¿å­˜"""
    if not supabase:
        print("âš ï¸ Supabaseæ¥ç¶šæƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    try:
        data = {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "category": category,
            "total_sales": total,
            "avg_price": avg,
            "median_price": median,
            "min_price": min_price,
            "top_keywords": top_keywords,
            "top_characters": top_characters
        }
        supabase.table("sales_data").insert(data).execute()
        print("âœ… Supabaseã¸ä¿å­˜å®Œäº†ï¼")
    except Exception as e:
        print(f"âš ï¸ Supabaseä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

# ===============================
# â‘¥ ãƒ‡ãƒ¼ã‚¿åˆ†æå‡¦ç†
# ===============================
def analyze_items(items):
    exclude_keywords = [
        "yugioh", "one piece", "weiss", "digimon",
        "dragon ball", "vanguard", "magic the gathering"
    ]

    filtered = []
    for item in items:
        title = item.get("title", "").lower()
        seller = item.get("seller", {}).get("username", "").lower()
        if (not any(ex in title for ex in exclude_keywords)) and ("japan" in seller or "japan" in title):
            filtered.append(item)

    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿æ•°: {len(filtered)} ä»¶")

    prices, titles = [], []
    for item in filtered:
        try:
            price = float(item["price"]["value"])
            if 1 <= price <= 20000:
                prices.append(price)
                titles.append(item.get("title", "").lower())
        except:
            continue

    if not prices:
        print("âš ï¸ æœ‰åŠ¹ãªè²©å£²ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚")
        return

    avg_price = np.mean(prices)
    median_price = np.median(prices)
    min_price = np.min(prices)
    max_price = np.max(prices)

    print("\nğŸ“ˆ ä¾¡æ ¼çµ±è¨ˆï¼ˆsortè§£é™¤ãƒ»è‡ªç„¶é †ï¼‰")
    print(f"å¹³å‡ä¾¡æ ¼: ${avg_price:.2f}")
    print(f"ä¸­å¤®å€¤: ${median_price:.2f}")
    print(f"æœ€ä½: ${min_price:.2f}, æœ€é«˜: ${max_price:.2f}")

    ignore_words = [
        "pokemon", "card", "japan", "tcg", "game",
        "rare", "set", "promo", "new", "used",
        "sealed", "edition", "japanese"
    ]
    words = []
    for title in titles:
        clean = re.sub(r"[^a-zA-Z0-9\s]", "", title)
        for w in clean.split():
            if len(w) > 2 and w not in ignore_words:
                words.append(w.lower())

    counter = Counter(words)
    print("\nğŸ”¥ å£²ã‚Œç­‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰TOP15")
    for word, count in counter.most_common(15):
        print(f"- {word.title()} : {count}ä»¶")

    targets = ["charizard", "pikachu", "mewtwo", "eevee", "gengar", "lugia", "rayquaza", "snorlax"]
    print("\nğŸ‰ ç‰¹å®šã‚«ãƒ¼ãƒ‰åˆ¥ã®è²©å£²å‚¾å‘")
    top_characters = {}
    for name in targets:
        related = [
            float(item["price"]["value"])
            for item in filtered if re.search(name, item.get("title", "").lower())
        ]
        if related:
            avg_val = float(np.mean(related))
            print(f"{name.title()} : {len(related)}ä»¶, å¹³å‡ ${avg_val:.2f}")
            top_characters[name] = {"count": len(related), "avg": avg_val}
        else:
            print(f"{name.title()} : è©²å½“ãªã—")
            top_characters[name] = {"count": 0, "avg": 0}

    # Supabaseã«ä¿å­˜
    save_sales_data(
        category="ãƒã‚±ãƒ¢ãƒ³ã‚«ãƒ¼ãƒ‰",
        total=len(filtered),
        avg=float(avg_price),
        median=float(median_price),
        top_keywords=dict(counter.most_common(15)),
        top_characters=top_characters
    )

# ===============================
# â‘¦ ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ===============================
if __name__ == "__main__":
    print("ğŸŒ eBay ãƒã‚±ãƒ¢ãƒ³ã‚«ãƒ¼ãƒ‰å¸‚å ´åˆ†æï¼ˆsortè§£é™¤ï¼‹Supabaseä¿å­˜å¯¾å¿œï¼‰")
    items = fetch_all_items(limit=100, max_pages=10)
    analyze_items(items)
